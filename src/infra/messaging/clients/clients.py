from confluent_kafka import Producer, Consumer
import orjson
import asyncio
import threading
from typing import Any
from src.core.dto.internal.mq import ConsumerConfigDomain, ProducerConfigDomain
from src.config.settings import kafka_settings
from src.infra.messaging.serializers.unified_serializer import (
    create_unified_serializers,
)


def _datetime_serializer(obj):
    """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ datetime ì²˜ë¦¬"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")


def default_value_serializer(value: Any) -> bytes:
    # orjson ì§ë ¬í™” (datetime ì²˜ë¦¬ í¬í•¨) - 3-5ë°° ë¹ ë¦„
    return orjson.dumps(value, default=_datetime_serializer)


def default_key_serializer(value: Any) -> bytes:
    # ë¬¸ìì—´ í‚¤ ìš°ì„ , ì•„ë‹ˆë¼ë©´ ê·¸ëŒ€ë¡œ bytesë¼ê³  ê°€ì •
    if isinstance(value, str):
        return value.encode("utf-8")
    if isinstance(value, (bytes, bytearray)):
        return bytes(value)
    # ê·¸ ì™¸ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©
    return str(value).encode("utf-8")


def default_value_deserializer(raw: bytes) -> Any:
    # orjson ì—­ì§ë ¬í™” - 2-4ë°° ë¹ ë¦„
    return orjson.loads(raw)


def default_key_deserializer(raw: bytes | None) -> Any | None:
    if raw is None:
        return None
    try:
        return raw.decode("utf-8")
    except Exception:
        return raw


def producer_config(**overrides: Any) -> dict[str, Any]:
    """Producer ì„¤ì •ì„ ProducerConfigDomain ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„± í›„ confluent-kafka í˜•ì‹ìœ¼ë¡œ ë³€í™˜.

    - ì„±ëŠ¥ ìµœì í™”: ì²˜ë¦¬ëŸ‰(throughput) ìš°ì„  ì„¤ì •
    - ProducerConfigDomainì„ ì‚¬ìš©í•˜ì—¬ íƒ€ì… ì•ˆì „ì„± ë³´ì¥
    - confluent-kafka ë§¤ê°œë³€ìˆ˜ ëª…ìœ¼ë¡œ ë³€í™˜
    """
    # ProducerConfigDomainìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” ì„¤ì • êµ¬ì„±
    cfg_domain = ProducerConfigDomain(
        # ê¸°ë³¸ ì—°ê²° ì„¤ì •
        bootstrap_servers=kafka_settings.BOOTSTRAP_SERVERS,
        security_protocol="PLAINTEXT",
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ì²˜ë¦¬ëŸ‰ ìš°ì„ )
        acks="all",  # ì²˜ë¦¬ëŸ‰ ìµœì í™” ("all"ë³´ë‹¤ ë¹ ë¦„)
        batch_size=131072,  # 128KB (ê¸°ë³¸ê°’ 16KBë³´ë‹¤ 8ë°° í¬ê²Œ)
        linger_ms=50,  # 50ms ëŒ€ê¸° (ë°°ì¹˜ íš¨ìœ¨ ê·¹ëŒ€í™”)
        compression_type="lz4",  # LZ4 ì••ì¶• (ë¹ ë¥¸ ì••ì¶•/í•´ì œ)
        # ì•ˆì •ì„± ë° ì„±ëŠ¥ ì„¤ì •
        enable_idempotence=True,
        retries=2147483647,  # ìµœëŒ€ ì¬ì‹œë„
        max_in_flight_requests_per_connection=5,  # ì„±ëŠ¥ ìµœì í™”
        request_timeout_ms=30000,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        delivery_timeout_ms=120000,  # 2ë¶„ ì „ì²´ íƒ€ì„ì•„ì›ƒ
        # ì§ë ¬í™” í•¨ìˆ˜
        value_serializer=default_value_serializer,
        key_serializer=default_key_serializer,
    )

    # confluent-kafka í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì  í‘œê¸°ë²• ì‚¬ìš©)
    # buffer.memoryëŠ” confluent-kafkaì—ì„œ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œì™¸
    cfg = {
        "bootstrap.servers": cfg_domain.bootstrap_servers,
        "security.protocol": cfg_domain.security_protocol,
        "acks": cfg_domain.acks,
        "batch.size": cfg_domain.batch_size,
        "linger.ms": cfg_domain.linger_ms,
        "compression.type": cfg_domain.compression_type,
        # "buffer.memory": cfg_domain.buffer_memory,  # confluent-kafkaì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
        "retries": cfg_domain.retries,
        "enable.idempotence": cfg_domain.enable_idempotence,
        "max.in.flight.requests.per.connection": cfg_domain.max_in_flight_requests_per_connection,
        "request.timeout.ms": cfg_domain.request_timeout_ms,
        "delivery.timeout.ms": cfg_domain.delivery_timeout_ms,
    }

    cfg.update(overrides)  # ì‚¬ìš©ì ì§€ì • ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
    return cfg


def consumer_config(**overrides: Any) -> dict[str, Any]:
    """Consumer ì„¤ì •ì„ ConsumerConfigDomain ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„± í›„ confluent-kafka í˜•ì‹ìœ¼ë¡œ ë³€í™˜.

    - confluent-kafka-python í˜¸í™˜ ì„¤ì •ë§Œ ì‚¬ìš©
    - ìµœì†Œí•œì˜ í•„ìˆ˜ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
    """
    # ConsumerConfigDomainìœ¼ë¡œ ê¸°ë³¸ ì„¤ì • êµ¬ì„± (í˜¸í™˜ì„± ìœ ì§€)
    cfg_domain = ConsumerConfigDomain(
        # ê¸°ë³¸ ì—°ê²° ì„¤ì •
        bootstrap_servers=kafka_settings.BOOTSTRAP_SERVERS,
        security_protocol="PLAINTEXT",
        group_id=kafka_settings.CONSUMER_GROUP_ID,
        # ì˜¤í”„ì…‹ ê´€ë¦¬
        enable_auto_commit=True,
        auto_offset_reset=kafka_settings.AUTO_OFFSET_RESET,
        auto_commit_interval_ms=5000,  # 5ì´ˆ ìë™ ì»¤ë°‹
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (confluent-kafka ë¯¸ì§€ì› ì†ì„±ë“¤ì€ ë„ë©”ì¸ì—ë§Œ ìœ ì§€)
        fetch_min_bytes=102400,  # ë„ë©”ì¸ í˜¸í™˜ì„± ìœ ì§€
        fetch_max_wait_ms=500,  # ë„ë©”ì¸ í˜¸í™˜ì„± ìœ ì§€
        max_poll_records=1000,  # ë„ë©”ì¸ í˜¸í™˜ì„± ìœ ì§€
        max_poll_interval_ms=300000,  # 5ë¶„ í´ë§ ê°„ê²©
        # ì„¸ì…˜ ê´€ë¦¬
        session_timeout_ms=30000,  # 30ì´ˆ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
        heartbeat_interval_ms=10000,  # 10ì´ˆ í•˜íŠ¸ë¹„íŠ¸
        # ì—­ì§ë ¬í™” í•¨ìˆ˜
        value_deserializer=default_value_deserializer,
        key_deserializer=default_key_deserializer,
    )

    # confluent-kafka í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì§€ì›ë˜ëŠ” ì†ì„±ë§Œ í¬í•¨)
    cfg = {
        # í•„ìˆ˜ ê¸°ë³¸ ì„¤ì •
        "bootstrap.servers": cfg_domain.bootstrap_servers,
        "security.protocol": cfg_domain.security_protocol,
        "group.id": cfg_domain.group_id,
        # ì˜¤í”„ì…‹ ê´€ë¦¬
        "enable.auto.commit": cfg_domain.enable_auto_commit,
        "auto.offset.reset": cfg_domain.auto_offset_reset,
        "auto.commit.interval.ms": cfg_domain.auto_commit_interval_ms,
        # ì„¸ì…˜ ê´€ë¦¬ (confluent-kafka ì§€ì›)
        "session.timeout.ms": cfg_domain.session_timeout_ms,
        "heartbeat.interval.ms": cfg_domain.heartbeat_interval_ms,
        "max.poll.interval.ms": cfg_domain.max_poll_interval_ms,
    }

    cfg.update(overrides)
    return cfg


class AsyncProducerWrapper:
    """confluent-kafka Producerì˜ ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ë˜í¼ - í ê¸°ë°˜ ì•„í‚¤í…ì²˜"""

    def __init__(self, config: dict, serializers: tuple) -> None:
        self.config = config
        self.value_serializer, self.key_serializer = serializers
        self.producer: Producer | None = None
        self._started = False

        # ë¹„ë™ê¸° ë©”ì‹œì§€ í
        self._message_queue: asyncio.Queue = asyncio.Queue()

        # ì „ìš© poll ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._poll_thread: threading.Thread | None = None
        self._shutdown_event = threading.Event()

        # ì½”ë£¨í‹´ íƒœìŠ¤í¬ ê´€ë¦¬
        self._producer_task: asyncio.Task | None = None

    async def start(self) -> None:
        """Producer ì‹œì‘ - í ì²˜ë¦¬ ì½”ë£¨í‹´ ë° poll ìŠ¤ë ˆë“œ ì‹œì‘"""
        if self._started:
            return

        # Producer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë™ê¸° í˜¸ì¶œì´ì§€ë§Œ ë¹ ë¦„)
        self.producer = Producer(self.config)

        # ì „ìš© poll ìŠ¤ë ˆë“œ ì‹œì‘ (delivery report ì²˜ë¦¬)
        self._poll_thread = threading.Thread(
            target=self._poll_worker, name="kafka-producer-poll", daemon=True
        )
        self._poll_thread.start()

        # ë©”ì‹œì§€ ì²˜ë¦¬ ì½”ë£¨í‹´ ì‹œì‘
        self._producer_task = asyncio.create_task(self._message_producer_worker())

        self._started = True

    async def stop(self) -> None:
        """Producer ì¢…ë£Œ - graceful shutdown"""
        if not self._started or not self.producer:
            return

        # 1. íì— ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
        await self._message_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸

        # 2. ë©”ì‹œì§€ ì²˜ë¦¬ ì½”ë£¨í‹´ ì¢…ë£Œ ëŒ€ê¸°
        if self._producer_task:
            await self._producer_task

        # 3. ë‚¨ì€ ë©”ì‹œì§€ flush (ë¹„ë™ê¸°)
        await asyncio.to_thread(self.producer.flush, 30.0)

        # 4. poll ìŠ¤ë ˆë“œ ì¢…ë£Œ
        self._shutdown_event.set()
        if self._poll_thread and self._poll_thread.is_alive():
            self._poll_thread.join(timeout=5.0)

        self.producer = None
        self._started = False

    async def send_and_wait(self, topic: str, value: Any, key: Any = None) -> None:
        """ë©”ì‹œì§€ ì „ì†¡ - íì— ì¶”ê°€ (ë…¼ë¸”ë¡œí‚¹)"""
        if not self._started or not self.producer:
            raise RuntimeError("Producer not started")

        # í†µí•© ì§ë ¬í™” ì²˜ë¦¬ (ë°”ì´íŠ¸/JSON ìë™ êµ¬ë¶„)
        serialized_value = self.value_serializer(value)
        serialized_key = self.key_serializer(key)

        # ë©”ì‹œì§€ë¥¼ íì— ì¶”ê°€ (ë¹„ë™ê¸°)
        message = {"topic": topic, "value": serialized_value, "key": serialized_key}
        await self._message_queue.put(message)

    async def _message_producer_worker(self) -> None:
        """ë©”ì‹œì§€ ì²˜ë¦¬ ì½”ë£¨í‹´ - íì—ì„œ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì™€ produce() í˜¸ì¶œ"""
        while True:
            try:
                # íì—ì„œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸° ëŒ€ê¸°)
                message = await self._message_queue.get()

                # ì¢…ë£Œ ì‹ í˜¸ í™•ì¸
                if message is None:
                    break

                # Producer.produce() í˜¸ì¶œ (ë…¼ë¸”ë¡œí‚¹)
                self.producer.produce(
                    topic=message["topic"],
                    value=message["value"],
                    key=message["key"],
                    callback=self._delivery_callback,
                )

                # í ì‘ì—… ì™„ë£Œ í‘œì‹œ
                self._message_queue.task_done()

            except Exception as e:
                # ì—ëŸ¬ ë¡œê¹… (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œê±° ì‚¬ìš©)
                print(f"Producer worker error: {e}")

    def _poll_worker(self) -> None:
        """ì „ìš© poll ìŠ¤ë ˆë“œ - delivery report ì²˜ë¦¬"""
        while not self._shutdown_event.is_set():
            try:
                if self.producer:
                    # ë…¼ë¸”ë¡œí‚¹ pollë¡œ delivery report ì²˜ë¦¬
                    self.producer.poll(0.1)  # 100ms íƒ€ì„ì•„ì›ƒ
                else:
                    threading.Event().wait(0.1)  # Producer ì—†ìœ¼ë©´ ëŒ€ê¸°
            except Exception as e:
                # ì—ëŸ¬ ë¡œê¹… (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œê±° ì‚¬ìš©)
                print(f"Poll worker error: {e}")

    def _delivery_callback(self, err, msg) -> None:
        """Delivery report ì½œë°± - poll ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œë¨"""
        if err is not None:
            # ì „ì†¡ ì‹¤íŒ¨ ë¡œê¹… (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œê±° + ë©”íŠ¸ë¦­ ì‚¬ìš©)
            topic = msg.topic() if msg else "unknown"
            key = msg.key() if msg else "unknown"
            print(f"Message delivery failed: {err} --> Topic: {topic}, Key: {key}")
        # ì„±ê³µí•œ ê²½ìš°ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬ (í•„ìš”ì‹œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘)


class AsyncConsumerWrapper:
    """confluent-kafka Consumerì˜ ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ë˜í¼ - ì „ìš© poll ìŠ¤ë ˆë“œ ê¸°ë°˜"""

    def __init__(
        self,
        topic: list[str],
        config: dict[str, Any],
        deserializers: tuple[Any, Any],
    ):
        self.topic = topic
        self.config = config
        self.value_deserializer, self.key_deserializer = deserializers
        self.consumer: Consumer | None = None
        self._started = False
        self._loop = None  # ì´ë²¤íŠ¸ ë£¨í”„ ì €ì¥

        # ë¹„ë™ê¸° ë©”ì‹œì§€ í
        self._message_queue: asyncio.Queue = asyncio.Queue(
            maxsize=1000
        )  # ë°±í”„ë ˆì…” ë°©ì§€

        # ì „ìš© poll ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._poll_thread: threading.Thread | None = None
        self._shutdown_event = threading.Event()

    async def start(self) -> None:
        """Consumer ì‹œì‘ - ì „ìš© poll ìŠ¤ë ˆë“œ ì‹œì‘"""
        if self._started:
            return

        self._started = True

        # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ ì €ì¥
        self._loop = asyncio.get_event_loop()
        print(f"ğŸ” ì´ë²¤íŠ¸ ë£¨í”„ ì €ì¥: {self._loop}")

        # Consumer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë™ê¸° í˜¸ì¶œì´ì§€ë§Œ ë¹ ë¦„)
        self.consumer = Consumer(self.config)
        self.consumer.subscribe(self.topic)

        # ì „ìš© poll ìŠ¤ë ˆë“œ ì‹œì‘ (ë©”ì‹œì§€ ìˆ˜ì‹  ì²˜ë¦¬)
        self._poll_thread = threading.Thread(
            target=self._poll_worker, daemon=True, name=f"poll-{self.topic}"
        )
        self._poll_thread.start()
        print(f"ğŸš€ Poll ìŠ¤ë ˆë“œ ì‹œì‘: {self._poll_thread.name}")

    async def stop(self) -> None:
        """Consumer ì¢…ë£Œ - graceful shutdown"""
        if not self._started or not self.consumer:
            return

        # 1. poll ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸
        self._shutdown_event.set()

        # 2. íì— ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
        try:
            self._message_queue.put_nowait(None)  # ë¹„ë™ê¸° ì´í„°ë ˆì´í„° ì¢…ë£Œ
        except asyncio.QueueFull:
            pass  # íê°€ ê°€ë“ ì°¨ë©´ ë¬´ì‹œ

        # 3. poll ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self._poll_thread and self._poll_thread.is_alive():
            self._poll_thread.join(timeout=5.0)

        # 4. Consumer ì¢…ë£Œ
        await asyncio.to_thread(self.consumer.close)
        self.consumer = None
        self._started = False

    def __aiter__(self):
        return self

    async def __anext__(self):
        """ë¹„ë™ê¸° ì´í„°ë ˆì´í„° - íì—ì„œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°"""
        if not self._started:
            raise RuntimeError("Consumer not started")

        # íì—ì„œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸° ëŒ€ê¸°)
        msg = await self._message_queue.get()
        # ì¢…ë£Œ ì‹ í˜¸ í™•ì¸
        if msg is None:
            raise StopAsyncIteration

        return msg

    def _poll_worker(self) -> None:
        """ì „ìš© poll ìŠ¤ë ˆë“œ - ë©”ì‹œì§€ ìˆ˜ì‹  ë° íì— ì „ë‹¬"""
        while not self._shutdown_event.is_set():
            try:
                if self.consumer:
                    # ë…¼ë¸”ë¡œí‚¹ pollë¡œ ë©”ì‹œì§€ ìˆ˜ì‹ 
                    raw_msg = self.consumer.poll(timeout=0.1)  # 100ms íƒ€ì„ì•„ì›ƒ
                    if raw_msg is None:
                        continue  # ë©”ì‹œì§€ ì—†ìŒ, ê³„ì† ëŒ€ê¸°

                    if raw_msg.error():
                        # ì—ëŸ¬ ë¡œê¹… (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œê±° ì‚¬ìš©)
                        print(f"Consumer error: {raw_msg.error()}")
                        continue

                    # ì •ìƒ ë©”ì‹œì§€ë¥¼ ì—­ì§ë ¬í™”
                    try:
                        deserialized_value = self.value_deserializer(raw_msg.value())
                        deserialized_key = self.key_deserializer(raw_msg.key())
                        message = {
                            "key": deserialized_key,
                            "value": deserialized_value,
                            "topic": raw_msg.topic(),
                        }
                        # ë¹„ë™ê¸° í”Œë ˆê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸° íì— ì ‘ê·¼
                        if self._loop and not self._loop.is_closed():
                            # call_soon_threadsafeë¡œ ì•ˆì „í•˜ê²Œ íì— ì¶”ê°€
                            self._loop.call_soon_threadsafe(
                                lambda: self._add_message_to_queue(message)
                            )

                    except Exception as e:
                        print(f"ì—­ì§ë ¬í™” ë˜ëŠ” í ì¶”ê°€ ì˜¤ë¥˜: {e}")

                else:
                    # Consumerê°€ ì—†ìœ¼ë©´ ëŒ€ê¸°
                    threading.Event().wait(0.1)

            except Exception as e:
                # ì—ëŸ¬ ë¡œê¹… (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œê±° ì‚¬ìš©)
                print(f"Poll worker error: {e}")

    def _add_message_to_queue(self, msg: dict) -> None:
        """ë©”ì‹œì§€ë¥¼ íì— ì•ˆì „í•˜ê²Œ ì¶”ê°€ (ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ í˜¸ì¶œ)"""
        try:
            self._message_queue.put_nowait(msg)
        except asyncio.QueueFull:
            # íê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±° í›„ ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
            try:
                self._message_queue.get_nowait()  # ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±°
                self._message_queue.put_nowait(msg)  # ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
            except asyncio.QueueEmpty:
                pass  # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¬´ì‹œ


def create_producer(**overrides: Any) -> AsyncProducerWrapper:
    """ë¹„ë™ê¸° Producer ë˜í¼ ìƒì„± - ì„±ëŠ¥ ìµœì í™” ProducerConfigDomain ê¸°ë°˜"""
    # ì„±ëŠ¥ ìµœì í™”ëœ confluent-kafka ì„¤ì • ìƒì„±
    cfg = producer_config(**overrides)

    # í†µí•© ì§ë ¬í™”ê¸° ì‚¬ìš© (ë°”ì´íŠ¸/JSON ìë™ êµ¬ë¶„)
    serializers = create_unified_serializers()

    return AsyncProducerWrapper(cfg, serializers)


def create_consumer(topic: list[str], **overrides: Any) -> AsyncConsumerWrapper:
    """ë¹„ë™ê¸° Consumer ë˜í¼ ìƒì„± - ì„±ëŠ¥ ìµœì í™” ConsumerConfigDomain ê¸°ë°˜"""
    # latest ì„¤ì • ê°•ì œ ì ìš© (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
    overrides.setdefault("auto.offset.reset", "latest")

    # ì„±ëŠ¥ ìµœì í™”ëœ confluent-kafka ì„¤ì • ìƒì„±
    cfg = consumer_config(**overrides)

    # ì—­ì§ë ¬í™” í•¨ìˆ˜ëŠ” ë³„ë„ë¡œ ì „ë‹¬ (ì„±ëŠ¥ ìµœì í™”ëœ ê¸°ë³¸ í•¨ìˆ˜ ì‚¬ìš©)
    deserializers = (default_value_deserializer, default_key_deserializer)

    return AsyncConsumerWrapper(topic, cfg, deserializers)
